---
title: "Airbnb Pricing Determinants in Europe"
author: "Yacine Montacer"
date: "25/09/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

For my end of studies mini-senior project, I found a dataset containing Airbnb rental data for European cities for over 51,000 listings. The dataset includes features such as the total listing price, room type, host status, amenities and location information which can be exploited to analyze these factors' relation to the Airbnb prices. For more information see: <https://www.kaggle.com/datasets/thedevastator/airbnb-price-determinants-in-europe?resource=download>

We have data for several European cities for both weekdays & week-ends. So let's first begin by importing all the different data into one aggregate set:

```{r importation, echo = T, results = 'hide'}
# data frame for storing dataset
combined_data <- data.frame()

# list of cities & data types
cities <- c("amsterdam", "athens", "barcelona", "berlin", "budapest", "lisbon", "london", "paris", "rome", "vienna")
data_types <- c("weekdays", "weekends")

# import data from each file into combined_data
for (city in cities) {
  for (data_type in data_types) {
    # file_path for CSV file
    file_path <- paste("data/", city, "_", data_type, ".csv", sep = "")
    
    # import CSV file
    city_data <- read.csv(file_path)
    
    # Add variables to identify city and data_type (weekend or week-day)
    city_data$city <- city
    city_data$data_type <- data_type
    
    # Import into combined data
    combined_data <- rbind(combined_data, city_data)
  }
}
```

Now let's check the top rows of the data to get an idea on what we're working with:

```{r head}
head(combined_data, 15)
summary(combined_data)
```

Now let's include the libraries we need to move forward: 

```{r libraries, echo = T, results = 'hide'}
library(ggplot2)
library(tidyr)
library(randomForest)
library(caret)
library(leaflet)
library(dplyr)
library(sf)
library(readr)
library(corrplot)
library(RColorBrewer)
library(ggplotify)
library(grid)
```

Let's do a data clean-up. I want to limit outliers by removing the rows with the highest and lowest 10% of listing price, turn our boolean text variables to integer binary variables, and create integer dummy variables for cities and for the listing being for weekends or weekdays.

```{r cleaning, echo = T, results = 'hide'}
# Convert text variables to boolean integers
combined_data$room_shared <- ifelse(combined_data$room_shared == "False", 0, 1)
combined_data$room_private <- ifelse(combined_data$room_private == "False", 0, 1)
combined_data$host_is_superhost <- ifelse(combined_data$host_is_superhost == "False", 0, 1)

# Create dummy variables to represent data_type
combined_data$for_weekends <- as.integer(combined_data$data_type == "weekends")
combined_data$for_weekdays <- as.integer(combined_data$data_type == "weekdays")

# Create dummy variable to represent full houses and apartments
combined_data$full_home <- as.integer(combined_data$room_type != "Private room")

# Add a dummy variable for each city
encoded_cities <- model.matrix(~ 0 + city, data = combined_data)
colnames(encoded_cities) <- sub("city", "", colnames(encoded_cities))
combined_data <- cbind(combined_data, encoded_cities)

# Remove 10% of outliers in terms of listing price both from the bottom and the top
percentile_10 <- quantile(combined_data$realSum, 0.1)
percentile_90 <- quantile(combined_data$realSum, 0.9)
filtered_data <- combined_data %>%
  filter(realSum >= percentile_10, realSum <= percentile_90)

original_data <- combined_data
combined_data <- filtered_data
```

Now let's summarize our transformed data

```{r summary}
summary(combined_data)
```

Now let's proceed with some basic exploratory data analysis just to get an idea on how the listing price varies

```{r exploratory_analysis}
# Price histogram
ggplot(combined_data, aes(x = realSum)) +
  geom_histogram(fill = "steelblue", bins = 30) +
  labs(x = "Price", y = "Frequency", title = "Distribution of Prices") +
  theme_minimal()

# Price by room type
ggplot(combined_data, aes(x = room_type, y = realSum)) +
  geom_boxplot(fill = "steelblue") +
  labs(x = "Room Type", y = "Price", title = "Price Variation by Room Type") +
  theme_minimal()

# Price vs. distance to metro
ggplot(combined_data, aes(x = metro_dist, y = realSum)) +
  geom_point(color = "steelblue") +
  labs(x = "Distance to Metro", y = "Price", title = "Price vs. Distance to Metro") +
  theme_minimal()

# Price by city
ggplot(combined_data, aes(x = city, y = realSum)) +
  geom_boxplot(fill = "steelblue") +
  labs(x = "City", y = "realSum", title = "Distribution of realSum by City") +
  theme_minimal()

# Price by data_type (weekends and weekdays)
ggplot(combined_data, aes(x = data_type, y = realSum)) +
  geom_boxplot(fill = "steelblue") +
  labs(x = "Data Type", y = "realSum", title = "Distribution of realSum by Date") +
  theme_minimal()
```

We can see that the listing price isn't normally distributed. And we can also see that entire homes and apartments are priced higher than private rooms, which themselves are priced higher that shared rooms. And we can see that distance to metro_stations is somewhat negatively correlated to the listing price. And we can see that there is significant variation between cities. but listing prices between week_days and weekends aren't very different.

Let's take a look at the number of listings per city in our sample

```{r colors, echo = T, results = 'hide'}
# Define the cities
cities <- c("amsterdam", "athens", "barcelona", "berlin", "budapest", "lisbon", "london", "paris", "rome", "vienna")

# Generate a color palette
n_colors <- length(cities)
color_palette <- brewer.pal(n_colors, "Set3")

# Create a named vector of colors
city_colors <- setNames(color_palette, cities)
```

```{r pie}
# Number of listings per city with numbers in the legend
whole_dataset_pie <- original_data %>%
  group_by(city) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = "", y = count, fill = city)) +
  geom_bar(stat = "identity", width = 1) +
  geom_text(aes(label = count), position = position_stack(vjust = 0.5), size = 2.5, color = "black") +  # Add labels to the bars
  coord_polar(theta = "y") +
  scale_fill_manual(values = city_colors) +
  labs(title = "Number of Listings per City (Whole Dataset)") +
  theme_void()  # Use theme_void to create a clear background

# Print the pie chart with numbers in the legend
print(whole_dataset_pie)
```

Let's try looking at these on a map

```{r map}
# Create an sf object
combined_sf <- st_as_sf(original_data, coords = c("lng", "lat"), crs = 4326)

# Create a base leaflet map
m <- leaflet() %>%
  addTiles(urlTemplate = "https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png") %>% # Use high-resolution tile source
  setView(lng = 10, lat = 50, zoom = 6) # Increased initial zoom level

# Add markers with different colors based on room_type
m <- m %>%
  addCircleMarkers(
    data = combined_sf,
    fillColor = ~case_when(
      room_type == "Entire home/apt" ~ "blue",
      room_type == "Private room" ~ "orange",
      room_type == "Shared room" ~ "red",
      TRUE ~ "blue" # Use a default color for other cases
    ),
    fillOpacity = 0.7, # Adjust opacity
    radius = 5, # Adjust marker size
    group = "Airbnb Listings", # Group for layer control
    popup = ~paste("City: ", city, "<br>Room Type: ", room_type) # Popup content
  )

# Add layer control for toggling layers on/off
m <- m %>%
  addLayersControl(overlayGroups = "Airbnb Listings", position = "topleft")

# Center the map and improve appearance
m <- m %>%
  setView(lng = 10, lat = 50, zoom = 6) %>%
  htmlwidgets::onRender("
    function(el, x) {
      setTimeout(function() {
        map.invalidateSize();
      }, 100);
    }
  ")

# Display the map
m
```

Let's proceed with a correlation matrix to get an idea on which explanatory variables correlate to the listing price, and also get an idea on the correlation between dependent variables.

```{r correlation_matrix}
# Correlation matrix
cor_matrix <- cor(combined_data[, c("realSum", "person_capacity", "cleanliness_rating", "guest_satisfaction_overall", "bedrooms", "dist", "metro_dist", "rest_index", "attr_index", "rest_index_norm", "attr_index_norm", "lng", "lat", "biz", "host_is_superhost", "room_shared", "room_private", "for_weekends", "for_weekdays", "full_home", "multi")])

# Plot correlation matrix
corrplot(cor_matrix, method = "color", type = "upper", order = "hclust", tl.cex = 0.7)

# Center and improve appearance
par(mar = c(1, 1, 1, 1))
```

I chose to implement a random forest algorithm to see if I can find non-linear relationships between pricing and the exploratory variables, since a conventional linear regression did not work on our aggregate data. I first executed the RF algorithm using all the numerical explanatory variables, then using feature importance analysis to remove variables that didn't correlate, and then in cases of collinearities in the correlation matrix, removing the variable with the least score on the feature importance analysis.


```{r random_forest}
# Independent variables
predictors <- c(
  "lat",
  "lng",
  "attr_index_norm",
  "dist",
  "bedrooms",
  "guest_satisfaction_overall",
  "barcelona", "london", "host_is_superhost", "multi", "biz", "amsterdam"
)

data_subset <- combined_data[, c(predictors, "realSum")]

# Split the data into a training set and a testing set
set.seed(123) # For reproducibility
sample_index <- sample(1:nrow(data_subset), 0.7 * nrow(data_subset))
train_data <- data_subset[sample_index, ]
test_data <- data_subset[-sample_index, ]

# Train the Random Forest model
rf_model <- randomForest(realSum ~ ., data = train_data, ntree = 500)

# Make predictions on the test set
predictions <- predict(rf_model, test_data)

# Evaluate the model
rmse <- sqrt(mean((test_data$realSum - predictions)^2))
mae <- mean(abs(test_data$realSum - predictions))

# Print the evaluation metrics
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Mean Absolute Error (MAE):", mae, "\n")
```

the RMSE & MAE are acceptable in comparison to our mean and median. So the model seems to be adequate

```{r rsquared_mape}
# Calculate R-squared
r_squared <- 1 - (sum((test_data$realSum - predictions)^2) / sum((test_data$realSum - mean(test_data$realSum))^2))
print(paste("R-squared (R²):", r_squared))

# Calculate MAPE
mape <- mean(abs((test_data$realSum - predictions) / test_data$realSum)) * 100
print(paste("Mean Absolute Percentage Error (MAPE):", mape, "%"))
```

And our R squared and MAPE values are acceptable, though they are not ideal.

```{r scatter_plot}
# Compare observed values and predicted values
plot(test_data$realSum, predictions, 
     xlab = "Observed Price",
     ylab = "Predicted Price",
     main = "Comparison of Observed and Predicted Prices",
     col = "blue",
     pch = 16)
     
# Add a diagonal reference line
abline(0, 1, col = "red")
```

Let's take a look at the residuals plot to see if there is any pattern we can see in the error terms

```{r residuals}
# Calculate residuals
residuals <- test_data$realSum - predictions

# Plot residuals against predicted values
plot(predictions, residuals,
     xlab = "Predicted Price",
     ylab = "Residuals",
     main = "Residual Plot",
     col = "blue",
     pch = 16)

# Add a horizontal reference line at y = 0
abline(h = 0, col = "red")
```

Let's look into feature importance to see if we have any redundant explanatory variables

```{r feature_importance}
library(randomForest)
library(caret)
# Create the feature importance plot
importance <- importance(rf_model)
varImpPlot(rf_model, pch = 19, col = "blue", bg = "white", main = "Feature Importance")
```

In conclusion, in our samples there is considerable variance between cities in terms of listing prices. Our model shows this with the significant correlation of price with longitudes and latitudes, which incidentally was more correlated than the city dummy variables. The price is also correlated to the indexes relating to distance to restaurants and attractions, but since they are correlated to each other we only used one for the random forest model to prevent collinearity. And the same idea applies to the number of bedrooms and the distance to city centers.

Let's perform K-fold cross-validation

```{r cross_validation}
# Create a data frame with only the selected predictors and the target variable (price)
data_subset <- combined_data[, c(predictors, "realSum")]

# Define the number of folds (K) for cross-validation
num_folds <- 5  # You can adjust this as needed

# Create a training control object for cross-validation
train_control <- trainControl(
  method = "cv",          # Use K-fold cross-validation
  number = num_folds,     # Number of folds
  verboseIter = TRUE      # Display progress
)

# Apply K-fold cross-validation to your existing rf_model
set.seed(123)  # For reproducibility
cv_results <- train(
  realSum ~ .,             # Formula for the target variable
  data = data_subset,     # Data frame
  method = "rf",          # Random forest method
  trControl = train_control,  # Training control
  tuneGrid = data.frame(mtry = 3)  # Adjust mtry as needed
)

# Print the cross-validation results, including metrics
print(cv_results)
```
As we can see the results of this test reinforce the initial testing of the random forest model.
